{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786f1a87",
   "metadata": {
    "id": "dae1e9e5",
    "outputId": "79c3407f-7b52-463d-9952-6e2fc16736af",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "train_data_path = \"~/aiffel/dktc/data/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e37cd6",
   "metadata": {
    "id": "cf8fde1e",
    "outputId": "499deb6c-40a0-495d-c357-6e7483794ede",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>야 . 절뚝이 왜 ? 똑바로 걸어다녀라 어려서 소아마비 앓아서 그래 절뚝절뚝 방아깨...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>씨 이번에 또 틀렸잖아 . 죄송합니다 . 다시 확인하겠습니다 . 동기인 씨는 한번 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>야 왜 쳐 ? 나 가만히 있었는데 아니 왜 치냐고 내가 거짓말하는거야 ? 왜 또 이...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>니가 드디어 아들을 낳으니 내가 속이 다 시원하다 . 네 아버지 . 내가 그동안 명...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>야 옆반에 걔 봤냐 ? 어제 전학 온애 ? ? 어 걸어 다니면서 맨날 얼굴 툭툭 튀...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data  label\n",
       "3136  야 . 절뚝이 왜 ? 똑바로 걸어다녀라 어려서 소아마비 앓아서 그래 절뚝절뚝 방아깨...      3\n",
       "2118  씨 이번에 또 틀렸잖아 . 죄송합니다 . 다시 확인하겠습니다 . 동기인 씨는 한번 ...      2\n",
       "1811  야 왜 쳐 ? 나 가만히 있었는데 아니 왜 치냐고 내가 거짓말하는거야 ? 왜 또 이...      3\n",
       "70    니가 드디어 아들을 낳으니 내가 속이 다 시원하다 . 네 아버지 . 내가 그동안 명...      3\n",
       "2609  야 옆반에 걔 봤냐 ? 어제 전학 온애 ? ? 어 걸어 다니면서 맨날 얼굴 툭툭 튀...      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.sample(frac = 1, random_state = 42)\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!가-힣ㄱ-ㅎㅏ-ㅣ]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def check_class(it):\n",
    "    if '협박' in it:\n",
    "        return 0\n",
    "    elif '갈취' in it:\n",
    "        return 1\n",
    "    elif '직장 내 괴롭힘' in it:\n",
    "        return 2\n",
    "    elif '기타 괴롭힘' in it:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "train_data['data'] = train_data['conversation'].apply(lambda it : preprocess_sentence(it))\n",
    "train_data['label'] = train_data['class'].apply(lambda it: check_class(it))\n",
    "\n",
    "train_data = train_data[['data', 'label']]\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b389ef57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:138: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:138: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_93/3596193378.py:138: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not \"\"]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "wordnet = {}\n",
    "with open(\"wordnet.pickle\", \"rb\") as f:\n",
    "\twordnet = pickle.load(f)\n",
    "\n",
    "\n",
    "# 한글만 남기고 나머지는 삭제\n",
    "def get_only_hangul(line):\n",
    "\tparseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',line)\n",
    "\n",
    "\treturn parseText\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif len(new_words) != 0:\n",
    "\t\tsentence = ' '.join(new_words)\n",
    "\t\tnew_words = sentence.split(\" \")\n",
    "\n",
    "\telse:\n",
    "\t\tnew_words = \"\"\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynomyms = []\n",
    "\n",
    "\ttry:\n",
    "\t\tfor syn in wordnet[word]:\n",
    "\t\t\tfor s in syn:\n",
    "\t\t\t\tsynomyms.append(s)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "\treturn synomyms\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "def random_deletion(words, p):\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\t\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\tif len(new_words) >= 1:\n",
    "\t\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\t\tcounter += 1\n",
    "\t\telse:\n",
    "\t\t\trandom_word = \"\"\n",
    "\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "\n",
    "\n",
    "def EDA(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\tsentence = get_only_hangul(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not \"\"]\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4) + 1\n",
    "\n",
    "\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\t# sr\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# ri\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# rs\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\t# rd\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]\n",
    "\trandom.shuffle(augmented_sentences)\n",
    "\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c10cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set augmentation parameters\n",
    "alpha_sr = 0.1\n",
    "alpha_ri = 0.1\n",
    "alpha_rs = 0.1\n",
    "p_rd = 0.1\n",
    "num_aug = 4\n",
    "\n",
    "\n",
    "# Apply text data augmentation\n",
    "def apply_augmentation(data, labels, num_aug):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "    for sentence, label in zip(train_data, labels):\n",
    "        augmented_sentences = EDA(sentence, alpha_sr, alpha_ri, alpha_rs, p_rd, num_aug)\n",
    "        augmented_labels.extend([label] * len(augmented_sentences))\n",
    "        augmented_data.extend(augmented_sentences)\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5ab95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_sentences, augmented_labels = apply_augmentation(train_data['data'], train_data['label'], num_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98517dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  label\n",
      "0  야 . 절뚝이 왜 ? 똑바로 걸어다녀라 어려서 소아마비 앓아서 그래 절뚝절뚝 방아깨...      3\n",
      "1  씨 이번에 또 틀렸잖아 . 죄송합니다 . 다시 확인하겠습니다 . 동기인 씨는 한번 ...      2\n",
      "2  야 왜 쳐 ? 나 가만히 있었는데 아니 왜 치냐고 내가 거짓말하는거야 ? 왜 또 이...      3\n",
      "3  니가 드디어 아들을 낳으니 내가 속이 다 시원하다 . 네 아버지 . 내가 그동안 명...      3\n",
      "4  야 옆반에 걔 봤냐 ? 어제 전학 온애 ? ? 어 걸어 다니면서 맨날 얼굴 툭툭 튀...      3\n"
     ]
    }
   ],
   "source": [
    "# 합쳐진 데이터 및 라벨 생성\n",
    "combined_data = train_data['data'].tolist() + augmented_sentences\n",
    "combined_labels = train_data['label'].tolist() + augmented_labels\n",
    "\n",
    "# 합쳐진 데이터와 라벨을 DataFrame으로 변환\n",
    "combined_df = pd.DataFrame({'data': combined_data, 'label': combined_labels})\n",
    "\n",
    "print(combined_df.head())  # 합쳐진 데이터셋 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca04e11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: 야 . 절뚝이 왜 ? 똑바로 걸어다녀라 어려서 소아마비 앓아서 그래 절뚝절뚝 방아깨비냐 ? 뚱뚱한 방아깨비네 사람이 아닌 곤충이었어 놀리지마 곤충이 말을 하네 제발 그만해\n",
      "Example 2: 씨 이번에 또 틀렸잖아 . 죄송합니다 . 다시 확인하겠습니다 . 동기인 씨는 한번 알려주면 끝인데 자네는 매번 알려줘도 이래 . 죄송하지만 팀장님한테 확인받고 한 작업입니다 . 뭐 그럼 내가 일부러 그런다는 거야 . 그건 아니지만 . 그런 식으로 일할거면 회사 관둬요 . 누가 보면 내가 괴롭힌다고 생각하겠어 그런 말씀은 지나치십니다 . 그럼 알아서 잘 하던가 . 눈치도 없이 . 다음부터는 다시 확인하겠습니다 . 팀장님도 말씀 가려서 해주세요 .\n",
      "Example 3: 야 왜 쳐 ? 나 가만히 있었는데 아니 왜 치냐고 내가 거짓말하는거야 ? 왜 또 이래 ? 야 얘가 왜 또 이러냐는데 이제 그만해 그만하래 웃기다 너 하루 이틀도 아니고 맨날 이런식으로 몰아가잖아 그냥 재밌어서 그래 장난이야 너한텐 장난이겠지\n",
      "Example 4: 니가 드디어 아들을 낳으니 내가 속이 다 시원하다 . 네 아버지 . 내가 그동안 명절마다 사돈에 안부 전화할때마다 네가 네 역할을 다하지 못해서 얼마나 창피한 줄 알았니 ? 얼굴도 들지도 못했다 . 아 아버지 . 하아 . 제가 더 만 고생이 심했어요 . 니가 드디어 아기를 가졌으니 다행이야 . 마침 아들이니 더욱 다행이고 . 네 아버지 . 진짜 이 와서 너무 행복해요 . 근데 너 시험관하느라 몇 년 허비 한 거 알지 ? 니 나이가 몇인데 빨랑 둘째 가져 . 아버지 . 저 진짜 힘들게 아기 가진거고 둘째 생각 없어요 . 네가 큰 딸인데 그게 무슨 소리야 ? 빨랑 둘째 가져라 . 애 키우느라 쉰다매 . 어차피 시간도 많은데 병원 다니면서 힘써라 . 아버지 . 애 키우는 것도 힘들어요 . 정말 .\n",
      "Example 5: 야 옆반에 걔 봤냐 ? 어제 전학 온애 ? ? 어 걸어 다니면서 맨날 얼굴 툭툭 튀는것처럼 흔들던데 ? 그니까 그거 겁나 웃기지않냐 ? 뒤에서 보고있으면 겁나 웃겨 다음에 옆에서 놀래켜봐 그럼 반응도 겁나 웃겨 어 다음에 해봐야겠다 그리고 뭔가 옆에가면 냄새도 좀 나는거 같지않냐 ? 냄새 ? 그건 잘 모르겠던데 . 그래서 걔네반 애들도 잘 안다가가는거잖아 아 그런거였어 ?\n"
     ]
    }
   ],
   "source": [
    "# Print a few examples of combined data text\n",
    "for idx, text in enumerate(combined_data[:5]):  \n",
    "    print(f\"Example {idx + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac35355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined data into training and testing sets\n",
    "X_train_combined, X_test_combined, Y_train_combined, Y_test_combined = train_test_split(\n",
    "    combined_data,\n",
    "    combined_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=combined_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f801e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine Accuracy: 0.8244949494949495\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 5, ngram_range = (1,3))\n",
    "X_train_tf = tfidf.fit_transform(X_train_combined)\n",
    "\n",
    "svc_model = LinearSVC(random_state = 0, tol = 1e-5)\n",
    "svc_model.fit(X_train_tf, Y_train_combined)\n",
    "\n",
    "\n",
    "X_test_tf = tfidf.transform(X_test_combined)\n",
    "\n",
    "svc_pred = svc_model.predict(X_test_tf)\n",
    "\n",
    "\n",
    "svc_accuracy = accuracy_score(Y_test_combined, svc_pred)\n",
    "\n",
    "print(\"Linear Support Vector Machine Accuracy:\", svc_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64c200",
   "metadata": {},
   "source": [
    "1) rs(random swap), rd(random deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "904b5f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_93/1606832301.py:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not \"\"]\n"
     ]
    }
   ],
   "source": [
    "def EDA(sentence, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\tsentence = get_only_hangul(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not \"\"]\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4) + 1\n",
    "\n",
    "\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\n",
    "\t# rs\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\t# rd\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]\n",
    "\trandom.shuffle(augmented_sentences)\n",
    "\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317bf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set augmentation parameters\n",
    "alpha_sr = 0.1\n",
    "p_rd = 0.1\n",
    "num_aug = 4\n",
    "\n",
    "\n",
    "# Apply text data augmentation\n",
    "def apply_augmentation(data, labels, num_aug):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "    for sentence, label in zip(train_data, labels):\n",
    "        augmented_sentences = EDA(sentence, alpha_rs, p_rd, num_aug)\n",
    "        augmented_labels.extend([label] * len(augmented_sentences))\n",
    "        augmented_data.extend(augmented_sentences)\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24722ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_sentences, augmented_labels = apply_augmentation(train_data['data'], train_data['label'], num_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f28cf619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  label\n",
      "0  야 . 절뚝이 왜 ? 똑바로 걸어다녀라 어려서 소아마비 앓아서 그래 절뚝절뚝 방아깨...      3\n",
      "1  씨 이번에 또 틀렸잖아 . 죄송합니다 . 다시 확인하겠습니다 . 동기인 씨는 한번 ...      2\n",
      "2  야 왜 쳐 ? 나 가만히 있었는데 아니 왜 치냐고 내가 거짓말하는거야 ? 왜 또 이...      3\n",
      "3  니가 드디어 아들을 낳으니 내가 속이 다 시원하다 . 네 아버지 . 내가 그동안 명...      3\n",
      "4  야 옆반에 걔 봤냐 ? 어제 전학 온애 ? ? 어 걸어 다니면서 맨날 얼굴 툭툭 튀...      3\n"
     ]
    }
   ],
   "source": [
    "# 합쳐진 데이터 및 라벨 생성\n",
    "combined_data = train_data['data'].tolist() + augmented_sentences\n",
    "combined_labels = train_data['label'].tolist() + augmented_labels\n",
    "\n",
    "# 합쳐진 데이터와 라벨을 DataFrame으로 변환\n",
    "combined_df = pd.DataFrame({'data': combined_data, 'label': combined_labels})\n",
    "\n",
    "print(combined_df.head())  # 합쳐진 데이터셋 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e0c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined data into training and testing sets\n",
    "X_train_combined, X_test_combined, Y_train_combined, Y_test_combined = train_test_split(\n",
    "    combined_data,\n",
    "    combined_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=combined_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e30db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine Accuracy: 0.8244949494949495\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 5, ngram_range = (1,3))\n",
    "X_train_tf = tfidf.fit_transform(X_train_combined)\n",
    "\n",
    "svc_model = LinearSVC(random_state = 0, tol = 1e-5)\n",
    "svc_model.fit(X_train_tf, Y_train_combined)\n",
    "\n",
    "\n",
    "X_test_tf = tfidf.transform(X_test_combined)\n",
    "\n",
    "svc_pred = svc_model.predict(X_test_tf)\n",
    "\n",
    "\n",
    "svc_accuracy = accuracy_score(Y_test_combined, svc_pred)\n",
    "\n",
    "print(\"Linear Support Vector Machine Accuracy:\", svc_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525a801",
   "metadata": {},
   "source": [
    "2) sr(synonym_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5e1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_93/2920791182.py:4: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not \"\"]\n"
     ]
    }
   ],
   "source": [
    "def EDA(sentence, alpha_sr=0.1, num_aug=5):\n",
    "\tsentence = get_only_hangul(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not \"\"]\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4) + 1\n",
    "\n",
    "\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\n",
    "\t# sr\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e160ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set augmentation parameters\n",
    "alpha_sr = 0.1\n",
    "num_aug = 4\n",
    "\n",
    "\n",
    "# Apply text data augmentation\n",
    "def apply_augmentation(data, labels, num_aug):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "    for sentence, label in zip(train_data, labels):\n",
    "        augmented_sentences = EDA(sentence, alpha_sr, num_aug)\n",
    "        augmented_labels.extend([label] * len(augmented_sentences))\n",
    "        augmented_data.extend(augmented_sentences)\n",
    "    \n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1216c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_sentences, augmented_labels = apply_augmentation(train_data['data'], train_data['label'], num_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c212f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  label\n",
      "0  야 . 절뚝이 왜 ? 똑바로 걸어다녀라 어려서 소아마비 앓아서 그래 절뚝절뚝 방아깨...      3\n",
      "1  씨 이번에 또 틀렸잖아 . 죄송합니다 . 다시 확인하겠습니다 . 동기인 씨는 한번 ...      2\n",
      "2  야 왜 쳐 ? 나 가만히 있었는데 아니 왜 치냐고 내가 거짓말하는거야 ? 왜 또 이...      3\n",
      "3  니가 드디어 아들을 낳으니 내가 속이 다 시원하다 . 네 아버지 . 내가 그동안 명...      3\n",
      "4  야 옆반에 걔 봤냐 ? 어제 전학 온애 ? ? 어 걸어 다니면서 맨날 얼굴 툭툭 튀...      3\n"
     ]
    }
   ],
   "source": [
    "# 합쳐진 데이터 및 라벨 생성\n",
    "combined_data = train_data['data'].tolist() + augmented_sentences\n",
    "combined_labels = train_data['label'].tolist() + augmented_labels\n",
    "\n",
    "# 합쳐진 데이터와 라벨을 DataFrame으로 변환\n",
    "combined_df = pd.DataFrame({'data': combined_data, 'label': combined_labels})\n",
    "\n",
    "print(combined_df.head())  # 합쳐진 데이터셋 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c410c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined data into training and testing sets\n",
    "X_train_combined, X_test_combined, Y_train_combined, Y_test_combined = train_test_split(\n",
    "    combined_data,\n",
    "    combined_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=combined_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97c6edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine Accuracy: 0.827455919395466\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 5, ngram_range = (1,3))\n",
    "X_train_tf = tfidf.fit_transform(X_train_combined)\n",
    "\n",
    "svc_model = LinearSVC(random_state = 0, tol = 1e-5)\n",
    "svc_model.fit(X_train_tf, Y_train_combined)\n",
    "\n",
    "\n",
    "X_test_tf = tfidf.transform(X_test_combined)\n",
    "\n",
    "svc_pred = svc_model.predict(X_test_tf)\n",
    "\n",
    "\n",
    "svc_accuracy = accuracy_score(Y_test_combined, svc_pred)\n",
    "\n",
    "print(\"Linear Support Vector Machine Accuracy:\", svc_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036777b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
